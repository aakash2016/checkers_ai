{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reward is just a number which makes sense relative to each other\n",
    "# s, a -->> r, s'\n",
    "\n",
    "### EPISODES\n",
    "# episode - represents one run of the game\n",
    "# RL agent will learn across many episodes\n",
    "# num of episodes is a hyperparameter..\n",
    "## PLAYING TIC TAC TOE IS AN EPISODIC TASK because you can play it again and again.\n",
    "## End of an episode -- Terminal State\n",
    "# Each episode is a fresh start\n",
    "\n",
    "## NOTES ON ASSIGNING REWARDS\n",
    "# We prgrammers are the coaches to an AI\n",
    "## Reward is something we give to the agent\n",
    "\n",
    "## Value Functions = expected value of your final reward.\n",
    "## we dont just thing about immediate rewards.\n",
    "## Delayed Rewards\n",
    "# Note in Credit Assignment Problem we asked what action we took in past that is leading to the rewards we are getting now,\n",
    "## In Delayed rewards -- how is the action I am doing now related to the potential reward I may receive in future..-- PLANNING\n",
    "\n",
    "## VALUE TELLS THE FUTURE GOODNESS OF A STATE\n",
    "# V(s) THE VALUE OF A STATE is a measure of future rewards we may receive being in that state.\n",
    "## Rewards are immediate.\n",
    "# WE CHOOSE ACTIONS BASED ON VALUES OF A STATE, NOT ON THE REWARD WE WILL GET BY GOING TO THAT STATE,\n",
    "\n",
    "## value is a fast and efficient way of searching a game tree.\n",
    "## 19683 possible states.\n",
    "## O(1) constant time lookup because a state directly looks up to a value.\n",
    "# V(s) = E[all future rewards|S(t) = s]\n",
    "# In TTT V(s) acn be interpreted as probability of winning after arriving in state s\n",
    "# After initialization of the value func. we have to update it, V(s) <- V(s) + alpha*(V(s') - V(s))\n",
    "# Note value of terminal state will never be updated,,\n",
    "## Since value function is not accurate we will use epsilon greedy strategy to make a tradeoff bw exploration and exploitaton..\n",
    "## For any particular state we are only going to update the values for the states that were in that episode.\n",
    "##we are moving V(s) closer to V(s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation ###\n",
    "import numpy as np\n",
    "length = 3\n",
    "\n",
    "## we will have to use OOP approach,\n",
    "## Two main objects the agent and the environment..\n",
    "## There will be two instances of agent interacting with a single instance of environment..\n",
    "def play_game(p1, p2, env, draw = False):\n",
    "\t## Loop untill the game is over..\n",
    "\tcurrent_player = None\n",
    "\twhile not env.game_over():\n",
    "\t\t## switch/alternate bw the players\n",
    "\t\tif current_player == p1: ## player1 will go first\n",
    "\t\t\tcurrent_player = p2\n",
    "\t\telse:\n",
    "\t\t\tcurrent_player = p1\n",
    "\n",
    "\t\t## Displaying the board func. to display what positions are currently occupied..\n",
    "\t\tif draw:\n",
    "\t\t\tif draw == 1 and current_player == p1: ## board only gets drawn once.\n",
    "\t\t\t\tenv.draw_board()\n",
    "\t\t\tif draw == 2 and current_player == p2:\n",
    "\t\t\t\tenv.draw_board()\n",
    "\n",
    "\t\t# we need current player to perform an action which updates the environment and hence the state\n",
    "\t\tcurrent_player.take_action(env)\t\n",
    "\n",
    "\t\t## update state histories of all the agents\n",
    "\t\tstate = env.get_state()\n",
    "\t\tp1.update_state_history(state)\n",
    "\t\tp2.update_state_history(state)\n",
    "\t\n",
    "\t#we need an update function that updates the agents internal estimate of the value fn\n",
    "\t# This is where the update eqn will go\n",
    "\t## The update fn has to accept the env as well since it has to query the most current rewards from the env.\n",
    "\tif draw:\n",
    "\t\tenv.draw_board() ## draw the board one last time to see who won.\n",
    "\t\n",
    "    ## Doing the value fn update, since the episode is now over.\n",
    "\tp1.update(env)\n",
    "\tp2.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the value fn V(s) for all states\n",
    "# 1 = for any state where the player wins\n",
    "# 0 = for any state where the player looses or its a draw\n",
    "# 0.5 otherwise\n",
    "def initialize_Vx(env, state_winner_triple):\n",
    "\tV = np.zeros(env.num_states)\n",
    "\tfor state, winner, ended in state_winner_triple:\n",
    "\t\tif ended:\n",
    "\t\t\tif winner == env.x:\n",
    "\t\t\t\tv = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tv = 0\n",
    "\t\telse:\n",
    "\t\t\tv = 0.5\n",
    "\t\tV[state] = v\n",
    "\treturn V\n",
    "\n",
    "def initialize_Vo(env, state_winner_triple):\n",
    "\tV = np.zeros(env.num_states)\n",
    "\tfor state, winner, ended in state_winner_triple:\n",
    "\t\tif ended:\n",
    "\t\t\tif winner == env.o:\n",
    "\t\t\t\tv = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tv = 0\n",
    "\t\telse:\n",
    "\t\t\tv = 0.5\n",
    "\t\tV[state] = v\n",
    "\treturn V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we need to ennumerate every possible state and assign the values\n",
    "# How to enumerate??\n",
    "## With Game Tree the problem is that we will same tree same tree more than once in the tree\n",
    "## Game Tree will lead to 9 factorial possible states which is much grater than 3^9\n",
    "\n",
    "## So we will use permutations of length N(Enumerating states recursively)\n",
    "## Function : get_state_hash_and_winner\n",
    "# returns a list of triples(state, winner, board)\n",
    "## state = configuration of the board as a hashed integer\n",
    "## winner is None if ended is false\n",
    "# ips: env, i, j i,j lie bw 0 and 2\n",
    "\n",
    "def get_state_hash_and_winner(env, i = 0, j = 0):\n",
    "\tresults = []\n",
    "\tfor v in (0, env.x, env.o):\n",
    "\t\tenv.board[i][j] = v\n",
    "\t\tif j == 2:\n",
    "\t\t\tif i == 2:\n",
    "\t\t\t\t## if i =2 and j=2 then we are filling last location in the board,\n",
    "\t\t\t\t## Which means now we should check for who the winner is.. and whether or not the game is ended..\n",
    "\t\t\t\t## if the board is full, collect the results and return\n",
    "\t\t\t\t## note that the board can even be filled with all 0's but that dosent mean game over. \n",
    "\t\t\t\t# it just means that my permutation is over.. \n",
    "\t\t\t\tstate = env.get_state()\n",
    "\t\t\t\tended = env.game_over(forced_recalc = True)\n",
    "\t\t\t\twinner = env.winner\n",
    "\t\t\t\tresults.append((state, winner, ended))\n",
    "\t\t\telse:\n",
    " \t\t\t\tresults += get_state_hash_and_winner(env, i+1, 0)\n",
    "\t\telse:\n",
    "\t\t\tresults += get_state_hash_and_winner(env, i, j+1)\t\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment Class\n",
    "# Thing which agent interacts with\n",
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((length, length))\n",
    "        self.x = -1 # Represents an x on board\n",
    "        self.o = 1 # Represents an o on board\n",
    "        self.winner = None\n",
    "        self.ended =  False ## Becomes True at the end of the game\n",
    "        self.num_states = 3**(length*length)\n",
    "        \n",
    "    def is_empty(self, i, j): ## Useful fn for agent because the agent needs to check if its valid position or not   \n",
    "        return self.board[i][j] == 0\n",
    "    \n",
    "    def reward(self, sym): # 1 if we win the game; ow = 0\n",
    "        ## a reward is given as feedback on every state transition\n",
    "        if not self.game_over():\n",
    "            return 0\n",
    "        ## sym is the symbol of the agent\n",
    "        # sym will be either self.x or self.o\n",
    "        return 1 if self.winner == sym else 0\n",
    "    \n",
    "    ## Representing states\n",
    "    ## Map each state to a number, so we can store value fn as a numpy array.\n",
    "    ## 19683 possible states' not too big\n",
    "    ## 3 possible symbols in each location we will covert ternay to decimal no's\n",
    "    def get_state(self):\n",
    "        h = 0\n",
    "        k = 0\n",
    "        for i in range(length):\n",
    "            for j in range(length):\n",
    "                if self.board[i][j] == 0:\n",
    "                    v = 0\n",
    "                elif self.board[i][j] == self.x:\n",
    "                    v = 1\n",
    "                elif self.board[i][j] == self.o:\n",
    "                    v = 2\n",
    "                h += (3**k)*v\t\t## Hashed integer\n",
    "                k += 1\n",
    "        return h\n",
    "    \n",
    "    def game_over(self, forced_recalc = False):\n",
    "        if not forced_recalc and self.ended:\n",
    "            return self.ended\n",
    "        ## Check for winner\n",
    "        ## row check\n",
    "        for i in range(length):\n",
    "            for player in (self.x, self.o):\n",
    "                if self.board[i].sum() == player*length:\n",
    "                    self.winner = player\n",
    "                    self.ended = True\n",
    "                    return True\n",
    "                \n",
    "        ## column check\n",
    "        for j in range(length):\n",
    "            for player in (self.x, self.o):\n",
    "                if self.board[:,j].sum() == player*length:\n",
    "                    self.winner = player\n",
    "                    self.ended = True\n",
    "                    return True\n",
    "                \n",
    "        ## Diagonal Check\n",
    "        # a. top left to bottom right\n",
    "        for player in (self.x, self.o):\n",
    "            if self.board.trace() == player*length:\n",
    "                self.winner = player\n",
    "                self.ended = True\n",
    "                return True\n",
    "                \n",
    "        # b. top right to bottom left\n",
    "            if np.fliplr(self.board).trace() == player*length:\n",
    "                self.winner = player\n",
    "                self.ended = True\n",
    "                return True\n",
    "            \n",
    "        ## Draw Check\n",
    "        if np.all((self.board == 0) ==  False):\n",
    "            # winner stays None\n",
    "            self.winner = None\n",
    "            self.ended = True\n",
    "            return True\n",
    "        \n",
    "        ## Game is not over\n",
    "        self.winner = None\n",
    "        return False\n",
    "    \n",
    "    def draw_board(self):\n",
    "        for i in range(length):\n",
    "            print('------------------')\n",
    "            for j in range(length):\n",
    "                print(' ', end=\"\")\n",
    "                if self.board[i][j] == self.x:\n",
    "                    print('x', end=\"\")\n",
    "                elif self.board[i][j] == self.o:\n",
    "                    print('o', end=\"\")\n",
    "                else:\n",
    "                    print(' ', end=\"\")\n",
    "            print('')        \n",
    "        print('------------------')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Agent Class-- everything that contains the AI\n",
    "# It has to interact with the environment\n",
    "class Agent:\n",
    "    def __init__(self, eps=0.1, alpha = 0.5):\n",
    "        self.eps = eps # probability of choosing random action instead of greedy\n",
    "        self.alpha = alpha ## The learning rate\n",
    "        self.verbose = False\n",
    "        self.state_history = []\n",
    "        \n",
    "    def setV(self, V):    \n",
    "        self.V = V\n",
    "        \n",
    "    def set_symbol(self, sym):\n",
    "        self.sym = sym\n",
    "        \n",
    "    def set_verbose(self, v):    \n",
    "        self.verbose = v\n",
    "        \n",
    "    def reset_history(self):## At the end of episode.\n",
    "        self.state_history = []\n",
    "        \n",
    "    ## with probability eps we will just do random action\n",
    "    def take_action(self, env):\n",
    "        # Choose an action based on epsilon greedy strategy\n",
    "        r = np.random.rand()\n",
    "        best_state = None\n",
    "        if r < self.eps:\n",
    "            if self.verbose:\n",
    "                print('Taking a random action')\n",
    "            possible_moves = []    \n",
    "            for i in range(length):\n",
    "                for j in range(length):\n",
    "                    if env.is_empty(i, j):\n",
    "                        possible_moves.append((i,j))\n",
    "            idx = np.random.choice(len(possible_moves)) # uniform probability\n",
    "            next_move = possible_moves[idx]            \n",
    "        else: ## Greedy part of epsilon greedy\n",
    "            pos2value = {}\n",
    "            next_move = None\n",
    "            best_value = -1\n",
    "            for i in range(length):\n",
    "                for j in range(length):\n",
    "                    if env.is_empty(i, j):            \n",
    "                        ## what is the state if we made this move\n",
    "                        env.board[i][j] = self.sym\n",
    "                        state = env.get_state()\n",
    "                        env.board[i][j] = 0 # we are just checking\n",
    "                        pos2value[(i,j)] = self.V[state]\n",
    "                        if self.V[state] > best_value:\n",
    "                            best_value = self.V[state]\n",
    "                            best_state = state\n",
    "                            next_move = (i,j)\n",
    "                            \n",
    "            # If verbose draw the board with values.                \n",
    "            if self.verbose:\n",
    "                print('Taking a greedy action')\n",
    "                ## PRINT THE BOARD WITH VALUES\n",
    "                for i in range(length):\n",
    "                    print('------------------')\n",
    "                    for j in range(length):\n",
    "                        if env.is_empty(i,j):\n",
    "                            #print the value\n",
    "                            print(\" %.2f|\" % pos2value[(i,j)], end=\"\")\n",
    "                        else:\n",
    "                            print(\"  \", end=\"\")\n",
    "                            if env.board[i,j] == env.x:\n",
    "                                print(\"x  |\", end=\"\")\n",
    "                            elif env.board[i,j] == env.o:\n",
    "                                print(\"o  |\", end=\"\")\n",
    "                            else:\n",
    "                                print(\"   |\", end=\"\")\n",
    "                    print(\"\")\n",
    "                print(\"------------------\")\n",
    "\n",
    "        # make the move\n",
    "        env.board[next_move[0], next_move[1]] = self.sym    \n",
    "                                \n",
    "    def update_state_history(self,s):\n",
    "        self.state_history.append(s)\n",
    "        \n",
    "    def update(self, env):    \n",
    "        ## Backtracking over all the states.\n",
    "        # we will only do this at the end of an episode.\n",
    "        # V(next_state) = reward, if its the most current state.\n",
    "        # update for the value of a state depends on the value of next state.\n",
    "        reward = env.reward(self.sym)\n",
    "        target = reward\n",
    "        for prev in reversed(self.state_history):\n",
    "            value = self.V[prev] + self.alpha*(target - self.V[prev])\n",
    "            self.V[prev] = value\n",
    "            target = value\n",
    "        self.reset_history()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def set_symbol(self, sym):\n",
    "        self.sym = sym\n",
    "        \n",
    "    def take_action(self, env):    \n",
    "        while True:\n",
    "            ## break if we make a legal move\n",
    "            move = input('enter coordinate (i,j) for your next move')\n",
    "            i,j = move.split(',')\n",
    "            i = int(i)\n",
    "            j = int(j)\n",
    "            if env.is_empty(i,j):\n",
    "                env.board[i][j] = self.sym\n",
    "                break\n",
    "        \n",
    "    def update_state_history(self,s):\n",
    "        pass     \n",
    "    \n",
    "    def update(self, env):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n",
      "5000\n",
      "5200\n",
      "5400\n",
      "5600\n",
      "5800\n",
      "6000\n",
      "6200\n",
      "6400\n",
      "6600\n",
      "6800\n",
      "7000\n",
      "7200\n",
      "7400\n",
      "7600\n",
      "7800\n",
      "8000\n",
      "8200\n",
      "8400\n",
      "8600\n",
      "8800\n",
      "9000\n",
      "9200\n",
      "9400\n",
      "9600\n",
      "9800\n"
     ]
    }
   ],
   "source": [
    "## Main fn\n",
    "if __name__ == '__main__':\n",
    "    # first step will be to create two AI's\n",
    "    p1 = Agent()\n",
    "    p2 = Agent()\n",
    "    \n",
    "    # Initiaizing the AI\n",
    "    # set initial V for p1 and p2\n",
    "    env = Env()\n",
    "    state_winner_triple = get_state_hash_and_winner(env)\n",
    "    \n",
    "    ## Initializing their value fns\n",
    "    Vx = initialize_Vx(env, state_winner_triple)\n",
    "    p1.setV(Vx)\n",
    "    Vo = initialize_Vo(env, state_winner_triple)\n",
    "    p2.setV(Vo)\n",
    "    \n",
    "    ## Give each player their symbol\n",
    "    p1.set_symbol(env.x)\n",
    "    p2.set_symbol(env.o)\n",
    "    \n",
    "## Training of AI    \n",
    "## Now we play 10k games of AI vs AI\n",
    "\n",
    "T = 10000\n",
    "for t in range(T):\n",
    "    if t%200 == 0:\n",
    "        print(t)\n",
    "    play_game(p1, p2, Env())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking a greedy action\n",
      "------------------\n",
      " 0.61| 0.49| 0.52|\n",
      "------------------\n",
      " 0.61| 0.66| 0.52|\n",
      "------------------\n",
      " 0.60| 0.65| 0.57|\n",
      "------------------\n",
      "------------------\n",
      "      \n",
      "------------------\n",
      "   x  \n",
      "------------------\n",
      "      \n",
      "------------------\n",
      "enter coordinate (i,j) for your next move2,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.62| 0.20| 0.65|\n",
      "------------------\n",
      " 0.68|  x  | 0.59|\n",
      "------------------\n",
      " 0.64| 0.65|  o  |\n",
      "------------------\n",
      "------------------\n",
      "      \n",
      "------------------\n",
      " x x  \n",
      "------------------\n",
      "     o\n",
      "------------------\n",
      "enter coordinate (i,j) for your next move1,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.05| 0.12| 0.63|\n",
      "------------------\n",
      "  x  |  x  |  o  |\n",
      "------------------\n",
      " 0.25| 0.25|  o  |\n",
      "------------------\n",
      "------------------\n",
      "     x\n",
      "------------------\n",
      " x x o\n",
      "------------------\n",
      "     o\n",
      "------------------\n",
      "enter coordinate (i,j) for your next move2,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.00| 0.12|  x  |\n",
      "------------------\n",
      "  x  |  x  |  o  |\n",
      "------------------\n",
      "  o  | 0.33|  o  |\n",
      "------------------\n",
      "------------------\n",
      "     x\n",
      "------------------\n",
      " x x o\n",
      "------------------\n",
      " o x o\n",
      "------------------\n",
      "enter coordinate (i,j) for your next move0,1\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.00|  o  |  x  |\n",
      "------------------\n",
      "  x  |  x  |  o  |\n",
      "------------------\n",
      "  o  |  x  |  o  |\n",
      "------------------\n",
      "------------------\n",
      " x o x\n",
      "------------------\n",
      " x x o\n",
      "------------------\n",
      " o x o\n",
      "------------------\n",
      "play_again? [Y/n]y\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.61| 0.49| 0.52|\n",
      "------------------\n",
      " 0.61| 0.66| 0.52|\n",
      "------------------\n",
      " 0.60| 0.65| 0.57|\n",
      "------------------\n",
      "------------------\n",
      "      \n",
      "------------------\n",
      "   x  \n",
      "------------------\n",
      "      \n",
      "------------------\n",
      "enter coordinate (i,j) for your next move0,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  o  | 0.50| 0.57|\n",
      "------------------\n",
      " 0.57|  x  | 0.64|\n",
      "------------------\n",
      " 0.61| 0.64| 0.51|\n",
      "------------------\n",
      "------------------\n",
      " o    \n",
      "------------------\n",
      "   x  \n",
      "------------------\n",
      "   x  \n",
      "------------------\n",
      "enter coordinate (i,j) for your next move0,1\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  o  |  o  | 0.55|\n",
      "------------------\n",
      " 0.06|  x  | 0.25|\n",
      "------------------\n",
      " 0.25|  x  | 0.06|\n",
      "------------------\n",
      "------------------\n",
      " o o x\n",
      "------------------\n",
      "   x  \n",
      "------------------\n",
      "   x  \n",
      "------------------\n",
      "enter coordinate (i,j) for your next move2,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  o  |  o  |  x  |\n",
      "------------------\n",
      " 0.18|  x  | 0.12|\n",
      "------------------\n",
      "  o  |  x  | 0.03|\n",
      "------------------\n",
      "------------------\n",
      " o o x\n",
      "------------------\n",
      " x x  \n",
      "------------------\n",
      " o x  \n",
      "------------------\n",
      "enter coordinate (i,j) for your next move1,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  o  |  o  |  x  |\n",
      "------------------\n",
      "  x  |  x  |  o  |\n",
      "------------------\n",
      "  o  |  x  | 0.00|\n",
      "------------------\n",
      "------------------\n",
      " o o x\n",
      "------------------\n",
      " x x o\n",
      "------------------\n",
      " o x x\n",
      "------------------\n",
      "play_again? [Y/n]n\n"
     ]
    }
   ],
   "source": [
    "## Play Human vs Agent\n",
    "human = Human()\n",
    "human.set_symbol(env.o)\n",
    "while True:\n",
    "    p1.set_verbose(True)\n",
    "    play_game(p1, human, Env(), draw = 2)\n",
    "    answer = input('play_again? [Y/n]')\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
